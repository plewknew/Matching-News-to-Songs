{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Lyrics_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWQIJsD8Mb-r",
        "colab_type": "text"
      },
      "source": [
        "## Purpose\n",
        "\n",
        "The purpose of this notebook is to display a working example of how to use Google's universal sentence encoder to compare two different strings. The general idea is to apply some basic NLP techniques with Spacy in order to increase the weights of the 'important' aspects of a sentence, then apply the sentence encoder to get a vector representation of the sentances. These sentances are later compared.\n",
        "\n",
        "In general, there will be one news article that is inputted. This will be compared to a dataframe of songs that have already been processed by the encoder. The one with the best cosine similarity will be selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j09eYuPYYWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e11ce967-c5ee-4594-a409-869743208183"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.9MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 10.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 11.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 12.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 10.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-rVtjR-Mb-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from newspaper import Article\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "import sentencepiece\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3-HoG0gMb-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed(text):\n",
        "    print('Start')\n",
        "    print('Starting embeddings...')\n",
        "    #embed_US = hub.Module(\"universal_sentence\")\n",
        "    embed_US = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
        "    embeddings = embed_US(text)\n",
        "    print('Extracting embeddings...')\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.tables_initializer())\n",
        "        embd = sess.run(embeddings)\n",
        "    dim_vector = ['Dim_{}'.format(i) for i in range(embd.shape[1])]\n",
        "    df_return = pd.DataFrame(embd, columns = dim_vector)\n",
        "    return df_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjfcYRqeMb-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #First we want to import the particular article that we want\n",
        "article = Article('https://www.voanews.com/usa/us-politics/sanders-still-wants-revolution-now-hes-got-company')\n",
        "article.download()\n",
        "article.parse()\n",
        "article_text=article.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhriVN-Mb-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now we import all of the song Lyrics (this should probably be done in another python script)\n",
        "df_songs = pd.DataFrame(pd.read_csv('songdata.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9T-v9LYMb-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0f4ea949-35b3-4ae2-a5fb-71d7516ee099"
      },
      "source": [
        "political_artists = ['Zac Brown Band',\n",
        "'Ziggy Marley',\n",
        "'The Beatles',\n",
        "'Arrogant Worms',\n",
        "'Billy Joel',\n",
        "'Bob Marley',\n",
        "'Coldplay',\n",
        "'Creedence Clearwater Revival',\n",
        "'Elton John',\n",
        "'Eminem',\n",
        "'Fleetwood Mac',\n",
        "'Garth Brooks',\n",
        "'John Denver',\n",
        "'Kanye West',\n",
        "'Linkin Park',\n",
        "'Lynyrd Skynyrd',\n",
        "'Rage Against The Machine',\n",
        "'Rascal Flatts',\n",
        "'Red Hot Chili Peppers',\n",
        "'System Of A Down',\n",
        "'Tragically Hip',\n",
        "'The White Stripes']\n",
        "#df_songs_political = df_songs[df_songs['artist'].isin(political_artists)]\n",
        "df_songs_political = df_songs\n",
        "df_songs_political.head()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>As Good As New</td>\n",
              "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
              "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang</td>\n",
              "      <td>/a/abba/bang_20598415.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang-A-Boomerang</td>\n",
              "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  artist  ...                                               text\n",
              "0   ABBA  ...  Look at her face, it's a wonderful face  \\nAnd...\n",
              "1   ABBA  ...  Take it easy with me, please  \\nTouch me gentl...\n",
              "2   ABBA  ...  I'll never know why I had to go  \\nWhy I had t...\n",
              "3   ABBA  ...  Making somebody happy is a question of give an...\n",
              "4   ABBA  ...  Making somebody happy is a question of give an...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA5kQwniMb-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_songs_political=df_songs_political.dropna(subset=['text'])\n",
        "df_songs_political['text'] = df_songs_political['text'].apply(lambda x: x.replace('\\n',' '))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pILJLzrMb_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_songs_politcal_lyrics= list(df_songs_political.iloc[:,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeDmLCdoMb_E",
        "colab_type": "text"
      },
      "source": [
        "## Here we begin to implement some of the NLP\n",
        "\n",
        "Implement stemming after"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5niVr4fMb_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This cell removes stop words and weights the description to focus on Nouns, Adjectives, and Verbs.\n",
        "def nlp_weighting(input_list):\n",
        "    print('Start')\n",
        "    nlp = spacy.load('en')\n",
        "    newtext = []\n",
        "\n",
        "    for doc in input_list:\n",
        "        nlpdoc=nlp(doc)\n",
        "        tempDoc=''\n",
        "        for token in nlpdoc:\n",
        "            if token.is_stop == False:\n",
        "                tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                if token.pos_ == 'NOUN':\n",
        "                    #We triple the strength of Nouns\n",
        "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                elif token.pos_ == 'PROPN':\n",
        "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                elif  token.pos_ == 'ADJ':\n",
        "                    #We double the strength fo Adjectives\n",
        "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                elif token.pos_ == 'VERB':\n",
        "                    #We double the strength of Verbs\n",
        "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
        "                    \n",
        "        #Here we HAD but I took it out a hard cutoff at 2100 characters. THis is because there were memory issues with the encoding otherwise\n",
        "        if len(tempDoc) < 110:\n",
        "            tempDoc =''\n",
        "\n",
        "        newtext.append(tempDoc)\n",
        "        \n",
        "    print('Returned')\n",
        "        \n",
        "    return(newtext)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w0NvpK6Mb_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "53d05782-81c7-46f9-f447-02679a978363"
      },
      "source": [
        "df_songs_politcal_lyrics = nlp_weighting(df_songs_politcal_lyrics)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Returned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTp2O36fMb_K",
        "colab_type": "text"
      },
      "source": [
        "Note below we have the version where we are taking TFIDF Weights. In reality, this would be harder to implement. In practice, we will use a pretrained model that will be able to return a vector to compare similarities. Also, we will want to restrict the size of our data in order to make comparisons feasible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Ojc5nCMb_P",
        "colab_type": "text"
      },
      "source": [
        "Implement embedding below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGk3kuskMb_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "760dcd43-2fcc-4706-d05a-abe5d3a8997f"
      },
      "source": [
        "len(df_songs_politcal_lyrics)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57650"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKBI4hR0Mb_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5733bec9-20dd-4c42-a890-3e462c7f49db"
      },
      "source": [
        "df_songs_political_lyrics_embed = embed(df_songs_politcal_lyrics)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Starting embeddings...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KkPjd62Mb_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_songs_political_lyrics_embed.to_csv('Lyric_embeddings.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuHdQFH6Mb_Y",
        "colab_type": "text"
      },
      "source": [
        "### Below we apply the NLP to the Article\n",
        "\n",
        "We will use the functions from above to do this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVHbnN63Mb_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bafab6c7-019c-453a-cec5-67286360d12e"
      },
      "source": [
        "article_text = nlp_weighting([article_text])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Returned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpyd4FETMb_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "19867d5f-cc2f-4052-a8de-eb4a9fb8dec1"
      },
      "source": [
        "article_text_embed = embed([article_text[0],'temp']).iloc[0,:]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Starting embeddings...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxkDnIRTMb_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5997a3ed-6fff-4184-9b1e-480f0212237a"
      },
      "source": [
        "max_cos = 0\n",
        "max_col = ''\n",
        "for i in range(len(df_songs_political_lyrics_embed)):\n",
        "    temp_cos = cosine_similarity([article_text_embed],[df_songs_political_lyrics_embed.iloc[i]])\n",
        "    if temp_cos > max_cos:\n",
        "        max_cos = temp_cos\n",
        "        max_col = df_songs_political.iloc[i]\n",
        "print(max_col)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "artist                                              The Jam\n",
            "song                                           In The Crowd\n",
            "link                      /j/jam/in+the+crowd_20068837.html\n",
            "text      When I'm in the crowd, I don't see anything   ...\n",
            "Name: 8973, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S6cuXQ4ed0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6e3bfc0c-f029-4c1e-e0ac-92d77af013c3"
      },
      "source": [
        "max_col[3]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"When I'm in the crowd, I don't see anything   My mind goes a blank, in the humid sunshine   When I'm in the crowd I don't see anything   I fall into a trance, at the supermarket   The noise flows me along, as I catch falling cans   Of baked beans on toast, technology is the most      And everyone seems just like me,   They struggle hard to set themselves free   And their waiting for the change      When I'm in the crowd, I can't remember my name   And my only link is a pint of walls ice cream   When I'm in the crowd - I don't see anything      Sometimes I think that its a plot   An equilibrium melting pot   The government sponsors underhand   When I'm in the crowd   When I'm in the crowd   When I'm in the crowd      And everyone seems that they're acting a dream   Cause they're just not thinking about each other   And they're taking orders, which are media spawned   And they should know better, now you have been warned      And don't forget you saw it here first   When I'm in the crowd   When I'm in the crowd   When I'm in the crowd      And life just simply moves along   In simple houses, simple jobs   And no ones wanting for the change   When I'm in the crowd   When I'm in the crowd   When I'm in the crowd  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}