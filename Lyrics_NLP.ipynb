{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "The purpose of this notebook is to display a working example of how to use Google's universal sentence encoder to compare two different strings. The general idea is to apply some basic NLP techniques with Spacy in order to increase the weights of the 'important' aspects of a sentence, then apply the sentence encoder to get a vector representation of the sentances. These sentances are later compared.\n",
    "\n",
    "In general, there will be one news article that is inputted. This will be compared to a dataframe of songs that have already been processed by the encoder. The one with the best cosine similarity will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from newspaper import Article\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text):\n",
    "    print('Start')\n",
    "    print('Starting embeddings...')\n",
    "    embed_US = hub.Module(\"universal_sentence\")\n",
    "    #embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "    embeddings = embed_US(text)\n",
    "    print('Extracting embeddings...')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        embd = sess.run(embeddings)\n",
    "    dim_vector = ['Dim_{}'.format(i) for i in range(embd.shape[1])]\n",
    "    df_return = pd.DataFrame(embd, columns = dim_vector)\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we want to import the particular article that we want\n",
    "article = Article('https://www.cbc.ca/news/world/cocaine-bust-philadelphia-ship-1.5180447')\n",
    "article.download()\n",
    "article.parse()\n",
    "article_text=article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we import all of the song Lyrics (this should probably be done in another python script)\n",
    "df_songs = pd.DataFrame(pd.read_csv('lyrics.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283031</th>\n",
       "      <td>283031</td>\n",
       "      <td>fight-music</td>\n",
       "      <td>2013</td>\n",
       "      <td>eminem-d12</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Chorus: Eminem]\\nThis kinda music\\nUse it and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283032</th>\n",
       "      <td>283032</td>\n",
       "      <td>keep-talkin</td>\n",
       "      <td>2008</td>\n",
       "      <td>eminem-d12</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Yeah, Detroit, motherfucka\\nDJ Green Lantern, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283033</th>\n",
       "      <td>283033</td>\n",
       "      <td>i-ll-shit-on-you</td>\n",
       "      <td>2008</td>\n",
       "      <td>eminem-d12</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>I'll shit on you, da da, da da, da da\\nI'll sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313003</th>\n",
       "      <td>313003</td>\n",
       "      <td>people-make-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>eminem</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313004</th>\n",
       "      <td>313004</td>\n",
       "      <td>my-darling</td>\n",
       "      <td>2009</td>\n",
       "      <td>eminem</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Ya look\\nIf I were to rap about the crap that'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index              song  year      artist    genre  \\\n",
       "283031  283031       fight-music  2013  eminem-d12  Hip-Hop   \n",
       "283032  283032       keep-talkin  2008  eminem-d12  Hip-Hop   \n",
       "283033  283033  i-ll-shit-on-you  2008  eminem-d12  Hip-Hop   \n",
       "313003  313003    people-make-me  2009      eminem  Hip-Hop   \n",
       "313004  313004        my-darling  2009      eminem  Hip-Hop   \n",
       "\n",
       "                                                   lyrics  \n",
       "283031  [Chorus: Eminem]\\nThis kinda music\\nUse it and...  \n",
       "283032  Yeah, Detroit, motherfucka\\nDJ Green Lantern, ...  \n",
       "283033  I'll shit on you, da da, da da, da da\\nI'll sh...  \n",
       "313003                                                NaN  \n",
       "313004  Ya look\\nIf I were to rap about the crap that'...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_artists = ['eminem-d12','eminem']#,'bob-marley' 'bob-marley-the-wailers','beyonce-knowles']\n",
    "df_songs_political = df_songs[df_songs['artist'].isin(political_artists)]\n",
    "df_songs_political.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_political=df_songs_political.dropna(subset=['lyrics'])\n",
    "df_songs_political['lyrics'] = df_songs_political['lyrics'].apply(lambda x: x.replace('\\n',' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_politcal_lyrics= list(df_songs_political.iloc[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we begin to implement some of the NLP\n",
    "\n",
    "Implement stemming after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell removes stop words and weights the description to focus on Nouns, Adjectives, and Verbs.\n",
    "def nlp_weighting(input_list):\n",
    "    print('Start')\n",
    "    nlp = spacy.load('en')\n",
    "    newtext = []\n",
    "\n",
    "    for doc in input_list:\n",
    "        nlpdoc=nlp(doc)\n",
    "        tempDoc=''\n",
    "        for token in nlpdoc:\n",
    "            if token.is_stop == False:\n",
    "                tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    #We triple the strength of Nouns\n",
    "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
    "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
    "                elif  token.pos_ == 'ADJ':\n",
    "                    #We double the strength fo Adjectives\n",
    "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
    "                elif token.pos_ == 'VERB':\n",
    "                    #We double the strength of Verbs\n",
    "                    tempDoc = tempDoc + ' ' + str(token.lemma_)\n",
    "                    \n",
    "        #Here we have a hard cutoff at 2100 characters. THis is because there were memory issues with the encoding otherwise\n",
    "        if len(tempDoc) > 2100:\n",
    "            tempDoc = tempDoc[0:2100]\n",
    "        if len(tempDoc) < 110:\n",
    "            tempDoc =''\n",
    "\n",
    "        newtext.append(tempDoc)\n",
    "        \n",
    "    print('Returned')\n",
    "        \n",
    "    return(newtext)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Returned\n"
     ]
    }
   ],
   "source": [
    "df_songs_politcal_lyrics = nlp_weighting(df_songs_politcal_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note below we have the version where we are taking TFIDF Weights. In reality, this would be harder to implement. In practice, we will use a pretrained model that will be able to return a vector to compare similarities. Also, we will want to restrict the size of our data in order to make comparisons feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement embedding below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_songs_politcal_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Starting embeddings...\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0918 16:15:31.952233 4428563904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    }
   ],
   "source": [
    "df_songs_political_lyrics_embed = embed(df_songs_politcal_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_political_lyrics_embed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we apply the NLP to the Article\n",
    "\n",
    "We will use the functions from above to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Returned\n"
     ]
    }
   ],
   "source": [
    "article_text = nlp_weighting([article_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Starting embeddings...\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 16:19:55.427418 4384302528 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0918 16:19:58.148826 4384302528 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    }
   ],
   "source": [
    "article_text_embed_temp = embed(['my fave color','temp sent']).iloc[0,:]\n",
    "print(article_text_embed_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text_embed = embed([article_text[0],'temp']).iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cos = 0\n",
    "max_col = ''\n",
    "for i in range(len(df_songs_political_lyrics_embed)):\n",
    "    temp_cos = cosine_similarity(article_text_embed,df_songs_political_lyrics_embed[i])\n",
    "    if temp_cos > max_cos:\n",
    "        max_cos = temp_cos\n",
    "        max_col = df_songs_political.iloc[i:]\n",
    "        print(max_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
